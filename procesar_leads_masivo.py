"""
Procesamiento masivo de leads con extracci√≥n de rese√±as y an√°lisis de pain points
Genera archivo final con mensajes personalizados para cada lead
"""

import pandas as pd
import time
from datetime import datetime
from gmaps_reviews_scraper import GoogleMapsReviewsScraper


# üéØ CONFIGURACI√ìN
# ================

# Archivo de entrada (tus leads)
LEADS_FILE = "LEADS_con_telefono_20251222_162340.xlsx"

# Filtros para seleccionar leads a procesar
FILTROS = {
    'ciudad': None,  # None = todas las ciudades, o "Medell√≠n", "Bogot√°", etc
    'rating_min': 3.5,  # Rating m√≠nimo
    'rating_max': 4.5,  # Rating m√°ximo (4.0-4.5 = pueden mejorar)
    'reviews_min': 0,  # M√≠nimo de rese√±as (0 = sin filtro)
}

# Cantidad de leads a procesar
MAX_LEADS = 100  # Cambia seg√∫n cu√°ntos quieras procesar (100 = ~8 horas)

# Rese√±as por lead
MAX_REVIEWS_PER_LEAD = 30  # 30 rese√±as por cl√≠nica

# Modo de navegador
HEADLESS = True  # True = oculto (m√°s r√°pido), False = visible (para debug)

# Pausa entre leads (segundos)
PAUSE_BETWEEN_LEADS = 8  # Pausa para evitar bloqueos de Google


def load_and_filter_leads(leads_file, filtros, max_leads):
    """Carga y filtra los leads seg√∫n criterios"""
    print("="*70)
    print("üìÇ CARGANDO Y FILTRANDO LEADS")
    print("="*70)

    # Cargar archivo
    print(f"\nüìÅ Archivo: {leads_file}")
    df = pd.read_excel(leads_file)
    print(f"‚úÖ {len(df)} leads cargados")

    # Aplicar filtros
    df_filtered = df.copy()

    # Filtro por ciudad
    if filtros['ciudad']:
        df_filtered = df_filtered[df_filtered['ciudad_busqueda'] == filtros['ciudad']]
        print(f"üìç Filtrado por ciudad '{filtros['ciudad']}': {len(df_filtered)} leads")

    # Filtro por rating
    df_filtered = df_filtered[
        (df_filtered['rating'] >= filtros['rating_min']) &
        (df_filtered['rating'] <= filtros['rating_max'])
    ]
    print(f"‚≠ê Filtrado por rating {filtros['rating_min']}-{filtros['rating_max']}: {len(df_filtered)} leads")

    # Filtro por m√≠nimo de rese√±as (solo si reviews_min > 0 y la columna existe)
    if filtros['reviews_min'] > 0 and 'reviews_count' in df_filtered.columns:
        df_filtered = df_filtered[df_filtered['reviews_count'] >= filtros['reviews_min']]
        print(f"üìä Filtrado por m√≠n {filtros['reviews_min']} rese√±as: {len(df_filtered)} leads")

    # Ordenar por rating (mejor primero)
    df_filtered = df_filtered.sort_values('rating', ascending=False)

    # Limitar cantidad
    df_filtered = df_filtered.head(max_leads)

    print(f"\nüéØ TOTAL A PROCESAR: {len(df_filtered)} leads")
    print("\nüìã Muestra:")
    print(df_filtered[['name', 'ciudad_busqueda', 'rating', 'reviews_count', 'phone']].head(10).to_string(index=False))

    return df_filtered


def process_leads_batch(df_leads, max_reviews_per_lead, headless, pause):
    """Procesa un lote de leads extrayendo rese√±as y analizando pain points"""
    print("\n" + "="*70)
    print("üöÄ INICIANDO PROCESAMIENTO MASIVO")
    print("="*70)

    total = len(df_leads)
    print(f"\nüìä Procesando {total} leads")
    print(f"‚è±Ô∏è  Tiempo estimado: {total * 3} - {total * 5} minutos")
    print(f"üïê Inicio: {datetime.now().strftime('%H:%M:%S')}\n")

    # Crear scraper
    scraper = GoogleMapsReviewsScraper(headless=headless)

    # Resultados
    results = []
    failed = []

    for idx, row in df_leads.iterrows():
        lead_num = len(results) + len(failed) + 1
        print(f"\n{'='*70}")
        print(f"üîç [{lead_num}/{total}] {row['name']}")
        print(f"{'='*70}")
        print(f"üìç {row['ciudad_busqueda']} | ‚≠ê {row['rating']} | üìä {row.get('reviews_count', '?')} rese√±as")

        try:
            # Extraer rese√±as
            reviews = scraper.extract_reviews_from_place(
                place_name=row['name'],
                city=row.get('ciudad_busqueda', ''),
                max_reviews=max_reviews_per_lead
            )

            if not reviews:
                print("‚ö†Ô∏è  No se pudieron extraer rese√±as")
                failed.append({
                    'name': row['name'],
                    'reason': 'No reviews extracted'
                })
                continue

            # Analizar pain points
            pain_points = scraper.analyze_pain_points(reviews)

            # Contar pain points por categor√≠a
            pain_counts = {k: len(v) for k, v in pain_points.items() if v}
            total_pain_points = sum(pain_counts.values())

            # Generar sugerencias de pitch
            pitches = scraper.generate_pitch_suggestions(pain_points)

            # Determinar el pain point principal
            top_pain_point = ""
            top_pain_count = 0
            suggested_message = ""

            if pain_counts:
                top_pain_point = max(pain_counts, key=pain_counts.get)
                top_pain_count = pain_counts[top_pain_point]

            if pitches:
                suggested_message = pitches[0]['suggested_pitch']

            # Guardar resultado
            result = {
                # Datos originales
                'nombre': row['name'],
                'ciudad': row['ciudad_busqueda'],
                'telefono': row['phone'],
                'rating': row['rating'],
                'reviews_count': row.get('reviews_count', 0),
                'direccion': row.get('address', ''),
                'website': row.get('website', ''),

                # An√°lisis de rese√±as
                'reviews_analizadas': len(reviews),
                'total_pain_points': total_pain_points,
                'pain_atencion_telefonica': pain_counts.get('atencion_telefonica', 0),
                'pain_demora_respuesta': pain_counts.get('demora_respuesta', 0),
                'pain_dificultad_agendar': pain_counts.get('dificultad_agendar', 0),
                'pain_horarios_limitados': pain_counts.get('horarios_limitados', 0),
                'pain_falta_seguimiento': pain_counts.get('falta_seguimiento', 0),
                'pain_info_precios': pain_counts.get('informacion_precios', 0),

                # Priorizaci√≥n
                'pain_point_principal': top_pain_point.replace('_', ' ').title(),
                'menciones_principal': top_pain_count,
                'prioridad': 'Alta' if total_pain_points >= 5 else 'Media' if total_pain_points >= 2 else 'Baja',

                # Pitch personalizado
                'mensaje_sugerido': suggested_message,

                # Metadata
                'fecha_analisis': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            results.append(result)

            # Mostrar resumen
            print(f"\n‚úÖ An√°lisis completado:")
            print(f"   ‚Ä¢ Rese√±as analizadas: {len(reviews)}")
            print(f"   ‚Ä¢ Pain points encontrados: {total_pain_points}")
            if top_pain_point:
                print(f"   ‚Ä¢ Problema principal: {top_pain_point.replace('_', ' ').title()} ({top_pain_count} menciones)")
            print(f"   ‚Ä¢ Prioridad: {result['prioridad']}")

            # Progreso
            progress = (lead_num / total) * 100
            remaining = total - lead_num
            print(f"\nüìä Progreso: {lead_num}/{total} ({progress:.1f}%) - Quedan {remaining}")

        except Exception as e:
            print(f"\n‚ùå Error procesando {row['name']}: {str(e)}")
            failed.append({
                'name': row['name'],
                'reason': str(e)
            })

        # Pausa entre leads
        if lead_num < total:
            print(f"\n‚è≥ Pausa de {pause}s antes del siguiente...")
            time.sleep(pause)

    # Cerrar scraper
    scraper.close()

    return results, failed


def save_results(results, failed):
    """Guarda los resultados en archivos Excel"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    print("\n" + "="*70)
    print("üíæ GUARDANDO RESULTADOS")
    print("="*70)

    if results:
        # Crear DataFrame
        df_results = pd.DataFrame(results)

        # Archivo completo
        filename_full = f'leads_con_analisis_{timestamp}.xlsx'
        df_results.to_excel(filename_full, index=False)
        print(f"\n‚úÖ Archivo completo: {filename_full}")
        print(f"   ({len(results)} leads procesados)")

        # Archivo solo alta prioridad
        df_high_priority = df_results[df_results['prioridad'] == 'Alta']
        if not df_high_priority.empty:
            filename_priority = f'leads_ALTA_PRIORIDAD_{timestamp}.xlsx'
            df_high_priority.to_excel(filename_priority, index=False)
            print(f"\n‚úÖ Alta prioridad: {filename_priority}")
            print(f"   ({len(df_high_priority)} leads con 5+ pain points)")

        # Archivo por pain point
        pain_categories = ['atencion_telefonica', 'demora_respuesta', 'dificultad_agendar']
        for category in pain_categories:
            col_name = f'pain_{category}'
            df_category = df_results[df_results[col_name] > 0].sort_values(col_name, ascending=False)
            if not df_category.empty:
                cat_name = category.replace('_', ' ').title()
                filename_cat = f'leads_pain_{category}_{timestamp}.xlsx'
                df_category.to_excel(filename_cat, index=False)
                print(f"\n‚úÖ Pain: {cat_name}: {filename_cat}")
                print(f"   ({len(df_category)} leads)")

        # Estad√≠sticas
        print("\n" + "="*70)
        print("üìä ESTAD√çSTICAS")
        print("="*70)
        print(f"\nTotal procesado: {len(results)}")
        print(f"Alta prioridad (5+ pain points): {len(df_results[df_results['prioridad'] == 'Alta'])}")
        print(f"Media prioridad (2-4 pain points): {len(df_results[df_results['prioridad'] == 'Media'])}")
        print(f"Baja prioridad (0-1 pain points): {len(df_results[df_results['prioridad'] == 'Baja'])}")

        print("\nPain points m√°s comunes:")
        for col in ['pain_atencion_telefonica', 'pain_demora_respuesta', 'pain_dificultad_agendar',
                    'pain_horarios_limitados', 'pain_falta_seguimiento', 'pain_info_precios']:
            total = df_results[col].sum()
            if total > 0:
                name = col.replace('pain_', '').replace('_', ' ').title()
                print(f"  ‚Ä¢ {name}: {int(total)} menciones")

        print("\nTop 10 leads con m√°s pain points:")
        top10 = df_results.nlargest(10, 'total_pain_points')[['nombre', 'ciudad', 'total_pain_points', 'pain_point_principal', 'prioridad']]
        print(top10.to_string(index=False))

    if failed:
        df_failed = pd.DataFrame(failed)
        filename_failed = f'leads_fallidos_{timestamp}.xlsx'
        df_failed.to_excel(filename_failed, index=False)
        print(f"\n‚ö†Ô∏è  Leads fallidos: {filename_failed}")
        print(f"   ({len(failed)} leads)")

    return timestamp


def main():
    """Funci√≥n principal"""
    print("\n" + "="*70)
    print("ü§ñ PROCESAMIENTO MASIVO DE LEADS")
    print("="*70)
    print("\nüìã Configuraci√≥n:")
    print(f"  ‚Ä¢ Archivo: {LEADS_FILE}")
    print(f"  ‚Ä¢ Ciudad: {FILTROS['ciudad'] or 'Todas'}")
    print(f"  ‚Ä¢ Rating: {FILTROS['rating_min']} - {FILTROS['rating_max']}")
    print(f"  ‚Ä¢ Min reviews: {FILTROS['reviews_min']}")
    print(f"  ‚Ä¢ Max leads: {MAX_LEADS}")
    print(f"  ‚Ä¢ Reviews por lead: {MAX_REVIEWS_PER_LEAD}")
    print(f"  ‚Ä¢ Navegador oculto: {HEADLESS}")

    # Auto-continuar (sin confirmaci√≥n)
    print("\n" + "="*70)
    print("‚úÖ Iniciando procesamiento autom√°tico...")
    # response = input("¬øContinuar con este procesamiento? (si/no): ").strip().lower()
    # if response != 'si':
    #     print("\n‚ùå Proceso cancelado")
    #     return

    # Cargar y filtrar leads
    df_leads = load_and_filter_leads(LEADS_FILE, FILTROS, MAX_LEADS)

    if df_leads.empty:
        print("\n‚ùå No hay leads que procesar con estos filtros")
        return

    # Auto-continuar (sin confirmaci√≥n)
    print("\n" + "="*70)
    tiempo_estimado_min = len(df_leads) * 3
    tiempo_estimado_max = len(df_leads) * 5
    print(f"‚è±Ô∏è  Tiempo estimado: {tiempo_estimado_min}-{tiempo_estimado_max} minutos")
    print(f"üïê Hora de inicio: {datetime.now().strftime('%H:%M:%S')}")
    print(f"\nüöÄ Iniciando procesamiento de {len(df_leads)} leads...")
    # response2 = input(f"\n¬øIniciar procesamiento de {len(df_leads)} leads? (si/no): ").strip().lower()
    # if response2 != 'si':
    #     print("\n‚ùå Proceso cancelado")
    #     return

    # Procesar
    start_time = time.time()
    results, failed = process_leads_batch(
        df_leads,
        MAX_REVIEWS_PER_LEAD,
        HEADLESS,
        PAUSE_BETWEEN_LEADS
    )

    # Guardar resultados
    timestamp = save_results(results, failed)

    # Resumen final
    elapsed_time = (time.time() - start_time) / 60
    print("\n" + "="*70)
    print("üéâ PROCESO COMPLETADO")
    print("="*70)
    print(f"\n‚è±Ô∏è  Tiempo total: {elapsed_time:.1f} minutos")
    print(f"‚úÖ Exitosos: {len(results)}")
    print(f"‚ùå Fallidos: {len(failed)}")
    print(f"\nüìÅ Archivos generados en la carpeta actual")
    print(f"üìÖ Timestamp: {timestamp}")

    print("\n" + "="*70)
    print("üìã PR√ìXIMOS PASOS:")
    print("="*70)
    print("\n1. Abre: leads_ALTA_PRIORIDAD_*.xlsx")
    print("2. Estos leads tienen 5+ quejas identificadas")
    print("3. Revisa la columna 'mensaje_sugerido'")
    print("4. Personaliza y env√≠a por WhatsApp")
    print("5. Empieza con 10-15 por d√≠a")
    print("\n‚úÖ ¬°Listo para prospectar!")


if __name__ == "__main__":
    main()
